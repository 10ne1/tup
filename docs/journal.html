<html>
<head>
<title>CS798 - Journal</title>
</head>
<body>
<h2>CS798 - Journal</h2>
<a href="proposal.html">Proposal</a><br>
<a href="..">Main page</a><br>
<h3>Milestones</h3>
<h4>Complete:</h4>
<ul>
  <li>Monitor - 2008/02/03</li>
  <li>Dependency generator (wrapper program and LD_PRELOAD library) 2008/02/13</li>
  <li>Reverse dependency file structure (SHA1 hash directories) 2008/02/17</li>
</ul>
<h4>Upcoming:</h4>
<ul>
  <li>Updater - 2008/03/15?</li>
  <li>Handle configuration files (eg: linux/.config) - 2008/04/01?</li>
  <li>Build small linux system with real source packages - 2008/04/20?</li>
  <li>Final presentation - 2008/04/30 - 10:30AM room 430A</li>
</ul>
<h3>Current Issues</h3>
<ul>
  <li>Reverse dependency files contain filenames dependent on the file in question. (eg: foo.hd contains the string "foo.c" if foo.c includes foo.h). When building foo.c, need to update foo.hd to make sure it contains one instance of "foo.c". Similarly, if foo.c is deleted, need to make sure "foo.c" is removed from foo.hd. Essentially need some sort of hash table that works efficiently in files (using open/read/write/seek).
    <ul>
       <li>Alternative - make foo.hd a directory, and put a link to foo.c in the directory?</li>
       <li><b>Feb 10 update</b>: The directory alternative seems to work quite well so far, with the added benefit that it is very easy to see the dependency 'database'. Still remains to be seen if it will work well with multiple sub-directories.</li>
        <li><b>Feb 17 update</b>: With a multiple subdirectory test (eg: with all source files in various 7-layer subdirectories), performance seemed to suffer. However, the directory structure can be flattened by using the SHA1 hash of the filename to generate a unique token. This prevents the reverse-dependency file structure from duplicating the entire program file structure multiple times, so fewer directories need to be created. Some readability is lost in that the dependencies can't easily be determined by doing 'ls' or 'find', but some simple scripts can be used to divine this information instead by reading the 'name' files in each hash directory, which effectively point back to the original filename.</li>
    </ul>
  <li>During compilation, need to update reverse dependency files of all included files. For a file that includes a large number of headers, this implies a large number of writes. How much does this adversely affect compilation?</li>
  <li>Is there a way to prove (by induction?) that the updater will work on all graphs, given that it works for some sample graphs (eg: a diamond, a chain, etc).</li>
</ul>
<h3>Weekly Status</h3>
<ul>
  <li><b>Pre-Jan 20</b>: <p>Investigated using <a href="http://en.wikipedia.org/wiki/Inotify">inotify</a> for the Monitor program. Initial results look promising, though there is a limit of 8192 watches that can be created. The number of watches is configurable by writing to a file in /proc, but the Monitor program won't know ahead of time whether or not this limit will be reached. For the initial implementation, 8192 will be sufficient for testing purposes (assuming 1 watch per directory). Currently the Monitor recursively watches a given directory, and displays file update notifications to the screen.</p></li>

  <li><b>Jan 27 - Feb 2</b>: <p>Monitor program finalized. Some simple tests were run, such as creating new directories and files, then deleting and re-creating them. The Monitor program is able to catch all file and directory modifications. Also started to work on the reverse dependency generator using the LD_PRELOAD method. This method works by loading a shared library before the invocation of gcc to intercept gcc's own filesystem calls (eg: fopen(), open(), creat()) to determine which files are being read/written to. Currently the shared library displays the files being accessed to the screen, as well as the mode (read or write). An initial test compiling a dummy C file that included one header shows the .c and .h file being read, and the .o file being written. Files in /tmp and files that are non-existent are ignored. The next step is to write out reverse dependency information (and possibly regular dependencies, as well?) to the disk. The first attempt at writing out reverse dependency information will be to use filesystem links (symlinks?) in directories.</p></li>
  <li><b>Feb 3 - Feb 9</b>: <p>Using the preloaded library to write out dependencies proved more difficult than initially anticipated after last weeks results. Last week I was testing by preloading the library and compiling a simple C file:</p>
  <pre>
  #include "lib.h"

  int main(void)
  {
     return libfunc();
  }
  </pre>
  By preloading the library I got the following output:
  <pre>
  $ LD_PRELOAD=/home/mjs/tup/ldpreload.so gcc -c main.c
  tup: Access file 'main.c' mode 0 from func open
  tup: Access file 'lib.h' mode 0 from func open
  tup: Access file 'main.o' mode 1 from func fopen64
  </pre>
  In theory at this point I could save the file accesses in a list, and then write out the reverse dependencies (eg: main.c and lib.h cause main.o to be updated). When I tried this, it seemed that the file accesses were going into separate lists. It turns out this is due to the fact that gcc spawns multiple sub-processes to handle the various tasks (pre-processing, compilation, and assembly). Each sub-process preloads its own version of the library, so the library is in fact initialized multiple times. This is evident after putting a constructor function in the library, and printing out the current PID with each file access:
  <pre>
  $ LD_PRELOAD=/home/mjs/tup/ldpreload.so gcc -c main.c
  Library init.
  Library init.
  Library init.
  tup[9162]: Access file 'main.c' mode 0 from func open
  tup[9162]: Access file 'lib.h' mode 0 from func open
  Library init.
  tup[9163]: Access file 'main.o' mode 1 from func fopen64
  </pre>
  As you can see, the files are read in one process and written to in another. In order to accumulate all the files in one process to write out the dep files, it would seem ideal to have a server process of some kind, and then all other invocations of the preloaded library could send their file accesses to the server. Then when the server quits, it could write out all of the dependencies. This is a bit difficult to do with just LD_PRELOAD, since gcc's use of execv() causes the library's destructor functions to not be called in all cases. A much simpler approach is to use a wrapper program that can setup the server and the LD_PRELOAD environment, call gcc, and then on shutdown write out the dependencies. The wrapper program output for the same example looks as follows:
  <pre>
  $ /home/mjs/tup/wrapper gcc -c main.c
  Started server '/tmp/tup-19142'
  tup-preload.so[19145]: Send file 'main.c' mode 0 from func open
  tup-server[19142]: Received file 'main.c' in mode 0
  tup-preload.so[19145]: Send file 'lib.h' mode 0 from func open
  tup-server[19142]: Received file 'lib.h' in mode 0
  tup-preload.so[19146]: Send file 'main.o' mode 1 from func fopen64
  tup-server[19142]: Received file 'main.o' in mode 1
  Stopping server '/tmp/tup-19142'
  </pre>
  <p>Now even though the files are read in one process (19145) and written to in another (19146), they are all sent through the socket to the main wrapper process (19142) and accumulated in a list.</p></li>

  <li><b>Feb 10 - Feb 16</b>: <p>There are still a few more tricks involved in getting the dependency information out. I extended the test case to include a lib.c file which also includes lib.h. This file is compiled and then archived into lib.a, and then both lib.a and main.o are built into the main executable (this is the same example from my <a href="proposal.html">proposal</a>). After the first run of using the list of files in the wrapper program to generate symlinks for dependencies, I ended up with the following file structure (and corresponding depgraph that was generated from a perl script):</p>
  <p><table border=1><tr>
  <td><pre>
  $ ls *.tupd
  lib.a.tupd:
  lib.a  main  stdKBDfV

  lib.c.tupd:
  lib.o

  lib.h.tupd:
  lib.o  main.o

  lib.o.tupd:
  lib.a  stdKBDfV

  main.c.tupd:
  main.o

  main.o.tupd:
  main

  main.tupd:
  main
  </pre></td>
  <td><img src="depgraph-1.png"></td>
  </tr></table></p>
  <p>There are a few obvious issues - first, some files are both read and written to in certain cases (such as linking) which causes files to be dependent on themselves (eg: 'main'). Further, even though the preloaded library ignores files in /tmp, there are still some temporary files created in the current directory by certain programs (in this case, 'ar'). Getting rid of the links for files dependent on themselves was a trivial strcmp(), but the temporary file issue was more difficult.</p>
  <p>What happens is 'ar' will write to the temporary file, and then issue a <em>rename()</em> call to move it to its final location (here, lib.a). Ideally the dependencies would be represented on disk without the intermediate temporary file. In other words, the process [read lib.o, write stdKBDfV, rename stdKBDfV to lib.a] should look the same as [read lib.o, write lib.a]. This is accomplished in the wrapper program handling rename events. The rename() libc call is wrapped by the preloader, similar to open() and fopen(). Instead of sending a single file event to the server, however, two file events are sent. This instructs the wrapper server to rename any existing files in its list to the new name. Then when the files are written out, all traces of the temporary file are gone. After these fixes, the following file structure and graph are obtained:</p>
  <p><table border=1><tr>
  <td><pre>
  $ ls *.tupd
  lib.a.tupd:
  main

  lib.c.tupd:
  lib.o

  lib.h.tupd:
  lib.o  main.o

  lib.o.tupd:
  lib.a

  main.c.tupd:
  main.o

  main.o.tupd:
  main
  </pre></td>
  <td><img src="depgraph-2.png"></td>
  </tr></table></p>
  <p>Note that although this graph is what we would expect to get if we were using standard gcc/make dependencies, they are in fact written in the opposite order, wherein the output file is listed in the input file's dependency directory. So if 'main.c' is changed, we can easily look in main.c.tupd to see that main.o needs to be updated. Similarly, we can look in main.o.tupd and see that main needs to be re-linked. No mention of any lib.* files are made in these directories, so they could be ignored in this case.</p>

  <li><b>Feb 17 - Feb 23</b>: <p>Per suggestion from Professor Lien, I constructed a test-case generator script that can generate random test cases. As input it takes the number of files to create, the number of headers the files should include, and the min/max directory hierarchy size. Currently all generated files are linked into a final executable (ie: there are no libraries here). One thing I tested with this script is the difference between having all files in a single directory, and all files in different directories that are 7 levels deep. Since the dependencies are stored using the paths of both objects, that means the dependency files would be 15 levels deep (1 extra for the filename itself). Using my benchmark program I compared the difference between using standard make with no dependency output and make using my wrapper program to generate dependencies. In each case there are 100.c files, each of which includes 7 other header files. I compared the amount of overhead in terms of execution time, as well as the disk space required. The execution time is measured by the factor increase over make without dependencies (so 1.05 means 5% extra time required).</p>
  <table border=1>
  <caption><i>Using the filenames to store dependencies vs. make with no dependency information.</i></caption>
  <tr>
     <td></td>
     <td bgcolor="#bbffff"><b>Single Directory</b></td>
     <td bgcolor="#bbffff"><b>7-Layers Deep</b></td>
  </tr>
  <tr>
     <td bgcolor="#bbffff"><b>Factor</b></td>
     <td>1.097951</td>
     <td>1.293445</td>
  </tr>
  <tr>
     <td bgcolor="#bbffff"><b>Disk blocks (du -sk)</b></td>
     <td>1208</td>
     <td>25004</td>
  </tr>
  </table>
  <p>As you can see, the number of directories severely impacts the performance of the wrapper. This is because the wrapper program must create large directory hierarchies, many of which are essentially duplicates of each other. This takes a long time to stat() and mkdir() each level of the directory. Also, the large number of empty directories consumes a lot of disk space.</p>
  <p>I recently started to use <a href="http://git.or.cz/">git</a> as a version control system for this project. Although I am still learning it, one of the things I found interesting is how it represents all kinds of objects (files, commits, etc) as SHA1 hashes in its database (which is also the filesystem). Using a similar approach here, instead of storing the dependency of a/b/foo.o on d/e/bar.h as ".tup/<font color="green"><b>d/e/bar.h</b></font>.tupd/<font color="blue"><b>a/b/foo.o</b></font>" I can take the SHA1 hash of the filenames to generate a unique ID for each file, and make that the directory. So here I would get ".tup/<font color="green"><b>4ebc353d14ff5a5bddd3fa79741ae7cc06719fd4</b></font>/<font color="blue"><b>0bc5dcc8490ad3f03010233ded74871358c9d349</b></font>". Although this is a bit more unreadable upon casual perusal (eg: using 'ls' or 'find'), it does have some nice properties. First, the depth of the dependency file tree is constant. Second, the objects in memory can easily be referenced by their 40-byte handle (20-byte SHA1 hash expanded as ASCII hex), instead of their filenames of arbitrary length (up to MAXPATHLEN, or 4096). This cuts down on the memory usage, since it is easier to allocate these objects on the stack. Using the same tests as before, I ran the benchmark program to determine the effect of using the SHA1 hash approach.</p>
  <table border=1>
  <caption><i>Using the SHA1-hash of the filename to store dependencies vs. make with no dependency information.</i></caption>
  <tr>
     <td></td>
     <td bgcolor="#bbffff"><b>Single Directory</b></td>
     <td bgcolor="#bbffff"><b>7-Layers Deep</b></td>
  </tr>
  <tr>
     <td bgcolor="#bbffff"><b>Factor</b></td>
     <td>1.116010</td>
     <td>1.141484</td>
  </tr>
  <tr>
     <td bgcolor="#bbffff"><b>Disk blocks (du -sk)</b></td>
     <td>1216</td>
     <td>1216</td>
  </tr>
  </table>
  <p>Although in the single directory case we can see a slight degradation in performance as compared to the direct filename approach, it is apparent that the SHA1 approach offers more consistent behavior regardless of the file structure of the program. In particular, note that the filespace required is exactly the same in both cases (that is not a copy/paste error!) This is much more desirable than the 20-times increase in disk space required by the direct filename approach when using sub-directories.</p>
  <p>Of course since the hash is a one-way algorithm, we need someway to get back from the hash name to the actual filename. In other words, convert the 0bc5d... name above into the filename "a/b/foo.o". This is currently done by creating a file called "name" in the corresponding 0bc5d... directory that contains the text "a/b/foo.o".</p>
  </li>

  <li><b>Feb 24 - Feb 30</b>: <p>I started work on the Updater portion of the build system. This required some modifications to the Monitor to get it to output changed file hashes to a special directory. For example, if you issue a "touch foo.c" command, the monitor will create a file called ".tup/attrib/e55780...". This is because 'touch' only affects file attributes (the timestamps). If the file is actually written to, it will go in the .tup/modify/ directory. I am not sure if the attrib directory will actually be necessary, but for now it is useful because I am used to doing 'touch foo.c; make' in order to test Makefiles, so I can do a similar test here.</p>
  <p>For now I have started to write the Updater as a Perl script, since it is easy to test several different approaches before I get into the memory management issues associated with C. The Updater currently only handles files being touched or modified, so it does not yet work if files are created, deleted or renamed. Note that although the Wrapper program handles when programs use rename() internally (such as ar), the Monitor and Updater will also need to be rename/delete aware (for example, if the user issues 'mv foo.c bar.c' or 'rm foo.c'. This has not yet been addressed.</p>
  <p>The algorithm used by the Updater is basically a depth-first search, using the nodes in the attrib/ and modify/ directories as the start nodes. The first stage of the algorithm walks the dependency graph stored in the filesystem to determine which nodes need to be updated (and abort if there is a circular dependency). The second stage walks the partial graph (again with DFS) to rebuild the nodes in their topological order.</p>
  <p>In order to see this in action, I modified the grapher script to display nodes that would be updated in red, with the rest of the graph in black. The updater script only reads the nodes in red. I also created a new script that can generate a random graph. Its input is the number of nodes and number of edges, and then it randomly picks two nodes and assigns an edge between them. The names of the nodes are picked randomly from words in /usr/share/dict/words. Here is one such generated graph, where the 'bioplasmic' file was touched:</p>
  <p><table border=1><caption><i>Randomly generated graph: Node 'bioplasmic' has been updated.</i></caption><tr><td><img src="depgraph-3.png"></td></tr></table></p>
  <p>The graph as seen by the Updater contains only the red nodes. All subtrees in black are not visited at all. Here is the graph generated from the Updater script:</p>
  <p><table border=1><caption><i>Partial DAG formed by the Updater</i></caption><tr><td><img src="depgraph-4.png"></td></tr></table></p>
  <p>The Updater program also outputs the commands that would run in order to update the tree. The files were generated using a simple test program called 'create_dep' which reads in N files, and writes to 1 file. When used with the wrapper, it essentially creates the edges of the graph in the .tup directory. The last argument is the file that is written - this can be seen as the file being "updated":</p>
  <pre>
  6713836dd56d9f3986dba8956c1774b11f04977c
  Visit: 6713836dd56d9f3986dba8956c1774b11f04977c
  Visit: 4292086f7b6bf5c87ec0d5dc39774b534c9ed9c1
  Visit: cbdb3db4b2645d5429755e26348aec5e7be16fad
  Visit: e2f3af40b23c178d018db610121a6e5723a7eed9
  Visit: adf3787e8363817082b2f720e7909a5895772f9d
  Visit: eb51fc12bda245d93ac07adcb045d0b6bafec313
  No cmd for .tup
  No cmd for 6713836dd56d9f3986dba8956c1774b11f04977c
  Skipping 4292086f7b6bf5c87ec0d5dc39774b534c9ed9c1 - count &gt;0
  Execute ../../create_dep foreknowing bioplasmic depressant
  Execute ../../create_dep depressant coquet stickwater
  Execute ../../create_dep bioplasmic stickwater conspecies
  Execute ../../create_dep stickwater spiketop
  Execute ../../create_dep bioplasmic pinchpenny finikin
  </pre>
  <p>So essentially what happens is it prints the start node (671383... = 'bioplasmic'), and then the nodes that it visits while building the partial DAG. During this construction phase, each node is assigned a count that describes the number of incoming edges. The second phase starts at the virtual ".tup" node, which has edges to each of the start nodes (here, there's only one). For each edge, it decrements the destination node's count by 1. If its count reaches 0, the command to update the destination node is executed, and it is recursively parsed. For example, since 'conspecies' has two incoming edges, the first time it is visited it is skipped (which is what the "Skipping ..." line means). Then the nodes are updated in the following order: 'depressant', 'stickwater', 'conspecies', 'spiketop', and 'finikin'.</p>
  <p>The Updater still needs to handle the other file changes (create/delete/move), and handle errors (eg: if updating a node fails, the Updater should be able to pick up where it left off).</p>
  </li>

</ul>
</body>
</html>
